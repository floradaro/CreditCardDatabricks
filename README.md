# ğŸ§  Proyecto: AnÃ¡lisis de Transacciones Financieras con PySpark y Delta Lake

## ğŸ“‹ DescripciÃ³n
Este proyecto implementa una arquitectura **Lakehouse** utilizando **Apache Spark** sobre **Databricks**, con el objetivo de procesar, limpiar y analizar datos de transacciones financieras.  
A travÃ©s de un flujo estructurado en tres capas â€”**Bronze**, **Silver** y **Gold**â€” se transforman los datos desde su forma cruda hasta obtener informaciÃ³n agregada y lista para el anÃ¡lisis.

---

## ğŸ—ï¸ Estructura del proyecto

### 1ï¸âƒ£ Capa Bronze
- Carga inicial de los datos en formato **Delta Lake**.
- ConversiÃ³n desde un archivo CSV a una tabla Delta persistente.
- VerificaciÃ³n de conteo y esquema.

### 2ï¸âƒ£ Capa Silver
- **Limpieza de datos:** eliminaciÃ³n de duplicados y nulos.
- **Transformaciones:** 
  - TipificaciÃ³n y conversiÃ³n de columnas.
  - CreaciÃ³n de nuevas variables derivadas (`Status`, `MontoCategoria`, `log_Amount`, `HoraAprox`, etc.).
- **Enriquecimiento:** uniÃ³n (*join*) con una tabla de usuarios simulada (`ref_usuarios`), agregando informaciÃ³n de edad, monto promedio y nivel de riesgo.

### 3ï¸âƒ£ Capa Gold
- **Agregaciones:** cÃ¡lculo de mÃ©tricas estadÃ­sticas por estado de transacciÃ³n y hora.
- **Indicadores resumen:** monto promedio, total, mÃ­nimo, mÃ¡ximo y desviaciÃ³n estÃ¡ndar por categorÃ­a.
- **AnÃ¡lisis temporal:** nÃºmero de transacciones y montos promedio por hora aproximada.
- **Persistencia:** almacenamiento de los resultados en tablas Delta (`gold_resumen_transacciones` y `gold_temporal`).

---

## âš™ï¸ TecnologÃ­as utilizadas

| TecnologÃ­a | DescripciÃ³n |
|-------------|-------------|
| ğŸ§© **Apache Spark (PySpark)** | Procesamiento distribuido de datos |
| ğŸª£ **Delta Lake** | Almacenamiento transaccional sobre Data Lake |
| â˜ï¸ **Databricks** | Plataforma para anÃ¡lisis y orquestaciÃ³n del pipeline |
| ğŸ **Python 3.x** | Lenguaje principal para ETL |
| ğŸ“Š **SQL / Spark SQL** | Consultas y agregaciones analÃ­ticas |

---

| SecciÃ³n | DescripciÃ³n |
|----------|--------------|
| **1. ImportaciÃ³n de librerÃ­as** | Carga de mÃ³dulos de PySpark y funciones SQL |
| **2. Capa Bronze** | Lectura del dataset, creaciÃ³n de tabla Delta y verificaciÃ³n |
| **3. Capa Silver** | Limpieza, derivaciÃ³n de columnas y join con usuarios |
| **4. Capa Gold** | Agregaciones y generaciÃ³n de indicadores finales |
| **5. Consultas SQL** | EjecuciÃ³n de queries analÃ­ticas en Databricks |
| **6. Visualizaciones** | AnÃ¡lisis temporal y comparativo |

---

## ğŸ’¾ Tablas generadas

| Tabla | DescripciÃ³n | Capa |
|--------|--------------|------|
| `lakehouse_fraude.bronze_transacciones` | Datos crudos convertidos a Delta | Bronze |
| `lakehouse_fraude.ref_usuarios` | Usuarios simulados con nivel de riesgo | Silver |
| `lakehouse_fraude.gold_resumen_transacciones` | MÃ©tricas globales de transacciones | Gold |
| `lakehouse_fraude.gold_temporal` | AnÃ¡lisis por hora y estado de transacciÃ³n | Gold |

## ğŸš€ CÃ³mo ejecutar el notebook

1. **Subir el archivo** a tu espacio de Databricks.  
2. **Conectar a un cluster** (Spark 3.x, Runtime compatible con Delta Lake).  
3. **Ejecutar las celdas** secuencialmente desde la capa Bronze hasta Gold.  
4. **Ver los resultados** en las tablas Delta creadas o con consultas SQL:


